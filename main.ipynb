{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coral_loss(source, target):\n",
    "    \"\"\" Deep CORAL loss to align feature distributions \"\"\"\n",
    "    d = source.size(1)  # Feature dimension\n",
    "    source_coral = torch.matmul((source - source.mean(dim=0)).T, (source - source.mean(dim=0))) / (source.size(0) - 1)\n",
    "    target_coral = torch.matmul((target - target.mean(dim=0)).T, (target - target.mean(dim=0))) / (target.size(0) - 1)\n",
    "    loss = torch.norm(source_coral - target_coral, p='fro')**2 / (4 * d**2)\n",
    "    return loss\n",
    "\n",
    "def new_domain_accuracy(model, new_domain_loader, device):\n",
    "    misclassified_images = []\n",
    "    misclassified_labels = []\n",
    "    misclassified_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in new_domain_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Find misclassified images\n",
    "            misclassified_idx = (predicted != labels).nonzero(as_tuple=True)[0]\n",
    "            for idx in misclassified_idx:\n",
    "                misclassified_images.append(images[idx].cpu())\n",
    "                misclassified_labels.append(labels[idx].cpu())\n",
    "                misclassified_preds.append(predicted[idx].cpu())\n",
    "        print(\"Accuracy for new domain: \"+str((1-len(misclassified_images)/len(new_domain_loader.dataset))*100))\n",
    "\n",
    "\n",
    "def train_domain_adaptation(model, source_loader, target_loader, criterion, optimizer, lambda_coral=0.1, epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for (source_images, source_labels), (target_images, _) in zip(source_loader, target_loader):\n",
    "            source_images, source_labels = source_images.to(device), source_labels.to(device)\n",
    "            target_images = target_images.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for source (labeled)\n",
    "            source_outputs = model(source_images)\n",
    "            classification_loss = criterion(source_outputs, source_labels)\n",
    "\n",
    "            # Forward pass for target (unlabeled)\n",
    "            source_features = model.forward_features(source_images)\n",
    "            target_features = model.forward_features(target_images)\n",
    "\n",
    "            # Flatten spatial features: [32, 512, 7, 7] â†’ [32, 512 * 7 * 7]\n",
    "            source_features = source_features.view(source_features.size(0), -1)\n",
    "            target_features = target_features.view(target_features.size(0), -1)\n",
    "            # Compute CORAL loss\n",
    "            coral_loss_val = coral_loss(source_features, target_features)\n",
    "\n",
    "            # Total loss = classification loss + CORAL loss\n",
    "            loss = classification_loss + lambda_coral * coral_loss_val\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute accuracy for source domain\n",
    "            _, predicted = torch.max(source_outputs, 1)\n",
    "            total_samples += source_labels.size(0)\n",
    "            correct_predictions += (predicted == source_labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(source_loader)\n",
    "        accuracy = correct_predictions / total_samples * 100\n",
    "        new_domain_accuracy(model, test_target_loader, device)\n",
    "        print(f\"TRAINING: Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%, CORAL Loss: {coral_loss_val.item():.4f}\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "data_dir = 'DAPlankton_subset\\\\CS'\n",
    "train_size = 0.2  # Percentage of data for training\n",
    "#test_size = 0.1   # Percentage of data for testing\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset from folder structure\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "train_len = int(len(dataset) * train_size)\n",
    "test_len = len(dataset) - train_len\n",
    "train_dataset, test_dataset = random_split(dataset, [train_len, test_len])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model creation\n",
    "num_classes = 3\n",
    "\n",
    "# Load pre-trained ResNet-18 model\n",
    "model = timm.create_model('resnet18', pretrained=True,num_classes=3)\n",
    "\n",
    "# Modify classifier for your dataset\n",
    "#model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "target_data_dir = 'DAPlankton_subset\\\\FC'  # Change this to your target domain folder\n",
    "target_dataset = datasets.ImageFolder(root=target_data_dir, transform=transform)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "train_len = int(len(target_dataset) * train_size)\n",
    "test_len = len(target_dataset) - train_len\n",
    "train_dataset, test_dataset = random_split(target_dataset, [train_len, test_len])\n",
    "\n",
    "# No labels needed, so we replace them with dummy labels\n",
    "train_nolabel_dataset = [(img, -1) for img, _ in train_dataset]\n",
    "\n",
    "train_target_loader = DataLoader(train_nolabel_dataset, batch_size=32, shuffle=True)\n",
    "test_target_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "train_domain_adaptation(model, train_loader, train_target_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
